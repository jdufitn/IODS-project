# Clustering and classification
This chapter focuses on performing and interpreting clustering and classification on the Boston data set.

## Load the Boston data set
The data are for the housing values in suburbs of Boston. The data are available from the **MASS package** and the variable descriptions can be found **[here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)**.

```{r warning = FALSE, message = FALSE}
library(MASS)
data("Boston") # load the data
str(Boston) # A data frame
dim(Boston) 
```

**Comment**: The data frame has 506 rows and 14 columns. All variables are numeric, with the variable **chas**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).


## A graphical overview of the data and summary of the variables

### Matrix plot of the variables

```{r fig.width = 10, fig.height = 6}
# plot matrix of the variables
pairs(Boston,
      col = "blue", # Change color
      pch = 18,    # Change shape of points
      main = "Matrix plot of the variables") # Add a main title
```


### The upper correlation matrix

```{r include = FALSE}
library(tidyr) # access the tidyr library
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) 
```

```{r include = FALSE} 
# the correlation matrix. NOTE: the matrix is not printed in the output because of include = FALSE command
cor_matrix %>%
round(digits = 2)
```

```{r message = FALSE, fig.align = "center"}
library(corrplot) # access the corrplot library
# visualize the upper correlation matrix
corrplot(cor_matrix, method="circle", type = "upper")
```

**Interpretations**:

* There seems to be a positive correlation between per capita crime rate by town **(crim)** and the index of accessibility to radial highways **(rad)** and also full-value property-tax rate per \$10,000 **(tax)**.

* A slightly positive correlation between per capita crime rate by town **(crim)** with proportion of non-retail business acres per town
**(indus)**, nitrogen oxides concentration (parts per 10 million) **(nox)**, and lower status of the population (percent) **(lstat)**.

* A positive correlation between the proportion of residential land zoned for lots over 25,000 square feet **(zn)** and the weighted mean of distances to five Boston employment centres **(dis)**.

* A positive correlation between the proportion of non-retail business acres per town **(indus)** with nitrogen oxides concentration (parts per 10 million) **(nox)**, proportion of owner-occupied units built prior to 1940 **(age)**, index of accessibility to radial highways **(rad)**, full-value property-tax rate per \$10,000 **(tax)**, and lower status of the population (percent) **(lstat)**.

* A positive correlation between average number of rooms per dwelling **(rm)** with median value of owner-occupied homes in \$1000s **(medv)**.

* A negative correlation between: lower status of the population (percent) **(lstat)** and median value of owner-occupied homes in \$1000s **(medv)**.

* Moreover, three variables are negatively correlated with the weighted mean of distances to five Boston employment centres **(dis)**. Those are proportion of owner-occupied units built prior to 1940 **(age)**, nitrogen oxides concentration (parts per 10 million) **(nox)**, and proportion of non-retail business acres per town **(indus)**.  

### Summary of the variables

```{r}
summary(Boston) 
```

**Interpretations**: On average,

* The per capital crime rate by the town is about 3.61.
* The proportion of residential land zoned for lots over 25,000 square feet is about 11.36.
* The proportion of non-retail business acres per town is about 11.14.
* The nitrogen oxides concentration (parts per 10 million) is about 0.55.
* The average number of rooms per dwelling is about 6.
* The proportion of owner-occupied units built prior to 1940 is about   68.57.
* The full-value property-tax rate per \$10,000 is about 408.2.
* The pupil-teacher ratio by town is about 18.46.
* The median value of owner-occupied homes in \$1000s is about 22.53.
* The **chas** Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). Its mean is about 0.069.


## Standardize the dataset

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# summaries of the scaled variables
summary(boston_scaled)
```

**How did the variables change?**: The variables have been rescaled to have a mean of zero and a standard deviation of one. 
For a standardized variable, each case's value on the standardized variable indicates it's difference from the mean of the original variable in number of standard deviations (of the original variable). 
```{r}
# Create a categorical variable of the crime rate

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# Drop the old crime rate variable from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# Divide the dataset to train and test sets

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```


## Fit the linear discriminant analysis on the train set

```{r fig.align = "center"}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1.3)

```


## Prediction of the classes with the LDA model on the test data

```{r}
# Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. DONE -- See above steps.

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
tbl_lda <- table(correct = correct_classes, predicted = lda.pred$class)
tbl_lda; rowSums(tbl_lda)
```

**Comments**: From the table, we see that the LDA predicts correctly:

* 16 out of 28 (i.e., 57.1%) low.
* 21 out of 26 (i.e., 80.8%) med_low.
* 18 out of 22 (i.e., 81.8%) med_high.
* All 26 (i.e., 100%) high.


## Reload the dataset, standardize and run k-means algorithm

```{r fig.align = "center"}
data("Boston") # Reload
Re_data <- scale(Boston) # standardize it

# distances between the observations
dist_eu <- dist(Re_data)

# k-means clustering with 3 clusters
km <- kmeans(Re_data, centers = 3)

# investigate what is the optimal number of clusters 

set.seed(123) # set the seed

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Re_data, k)$tot.withinss})

# visualize the results
library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

**Comment**: The "scree plot" above helps to identify the appropriate number of clusters. The "elbow shape" suggests that two clusters (k = 2) is the potential candidate, since the total WCSS drops radically.

```{r fig.width = 10, fig.height = 6}
# run the algorithm again

# k-means clustering
km_new <- kmeans(Re_data, centers = 2)

# plot the Re_scale Boston dataset with 2 clusters
pairs(Re_data, col = km_new$cluster)

table(km_new$cluster) 
```

**Comment**: With **k = 2**, clusters consist of 329 observations out of 506 in cluster 1, and cluster 2, 177 observations. The clusters are also separated by colour within the predictors. Some variables present a clear cut of observations, other it is a quite mix.  


## Bonus

```{r warning = FALSE, message = FALSE}
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

library(plotly) # access plotly library

# 3D plot of the columns of the matrix 
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', surfacecolor = train$crime)

# another 3D plot with color defined by the clusters of the k-means
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', surfacecolor = km_new$cluster)
```

**How do the plots differ? Are there any similarities?**: The two plots look relatively similar with a clear cut between clusters, and the number of observation within each group seems to be the same.
